{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inspirational Text Genration Model",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinnyvalenz/Inspirational-Quote-Generator/blob/RBasu/Inspirational_Text_Genration_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "Import required GPT-2 Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_"
      },
      "source": [
        "//Rohit Basu\n",
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS"
      },
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "\n",
        "Download prefered GPT2 Model Size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7838b5a8-2cd4-47fc-a94a-6bb4f867fc44"
      },
      "source": [
        "\n",
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 225Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 1.41Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 373Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [04:19, 1.92Mit/s]\n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 264Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 1.93Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 1.99Mit/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN"
      },
      "source": [
        "## Mounting Google Drive\n",
        "\n",
        "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n",
        "\n",
        "Running this cell (which will only work in Colaboratory) will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75aedc96-4646-49c0-e92c-f667b65171ca"
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll"
      },
      "source": [
        "file_name = \"Motivational-Quotes-Database.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeeSKtNWUedE"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS"
      },
      "source": [
        "gpt2.copy_file_from_gdrive(file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3"
      },
      "source": [
        "FINE TUNING\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhi1Roz7XFaX"
      },
      "source": [
        "Model Size: 124M: Default Learning Rate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32e463aa-902e-419f-ffbc-3a5a5f7adcb8"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='124M',=\n",
        "              steps=1000,\n",
        "              restore_from='fresh',\n",
        "              run_name='run1',\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=500\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  6.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 2302654 tokens\n",
            "Training...\n",
            "[10 | 27.91] loss=2.79 avg=2.79\n",
            "[20 | 50.04] loss=2.27 avg=2.53\n",
            "[30 | 72.91] loss=2.34 avg=2.46\n",
            "[40 | 96.92] loss=2.27 avg=2.41\n",
            "[50 | 121.59] loss=2.29 avg=2.39\n",
            "[60 | 145.26] loss=2.45 avg=2.40\n",
            "[70 | 168.87] loss=2.39 avg=2.40\n",
            "[80 | 192.96] loss=2.44 avg=2.40\n",
            "[90 | 217.02] loss=2.54 avg=2.42\n",
            "[100 | 240.84] loss=2.27 avg=2.40\n",
            "[110 | 264.70] loss=2.24 avg=2.39\n",
            "[120 | 288.76] loss=2.14 avg=2.37\n",
            "[130 | 312.82] loss=2.31 avg=2.36\n",
            "[140 | 336.81] loss=2.28 avg=2.36\n",
            "[150 | 360.76] loss=2.36 avg=2.36\n",
            "[160 | 384.65] loss=2.38 avg=2.36\n",
            "[170 | 408.54] loss=2.39 avg=2.36\n",
            "[180 | 432.45] loss=2.20 avg=2.35\n",
            "[190 | 456.40] loss=2.04 avg=2.33\n",
            "[200 | 480.37] loss=2.38 avg=2.34\n",
            "======== SAMPLE 1 ========\n",
            "'m only a dog, not a man, that's got to change.;Robert Shekauff;agenda\n",
            "5353;I think I have the best intentions. I want it to be an honor, something to work for, be an award and something to be remembered.;Evelyn Farr;agenda\n",
            "5354;We want to see the whole family make an impact on the world.;Bill Murray;agenda\n",
            "5355;I don't know how to look at things and say, 'Let's just have an amazing year. That'll be great.' And then it turns out to be absolutely insane.;J.K. Rowling;agenda\n",
            "5356;I think this whole business business, which is where I'm from now, is, 'I want the best of the best.' It's a business that has been around forever. It's still getting built, it's still evolving.;Ben Affleck;agenda\n",
            "5357;In your life you've got to have fun with it. You've got to have a sense of humor. You've got to have something that is funny and a bit funny. It's not going to have to be fun for you. That's why I love it in the end, as it's only been in a week and it's not gonna be in the past.;Ben Affleck;agenda\n",
            "5358;It's an interesting idea that people want to do something with their lives, to be the first person to have the chance to say, 'This is how I really feel about my family.;J. K. Rowling;agenda\n",
            "5359;I want to be the greatest mother ever alive, to help my kids and my kids are a blessing to me.;Mary Allen;agenda\n",
            "5360;My goal is to be a better mother. Maybe that's why I didn't have my best friend. She just got married.;Robert Shekauff;agenda\n",
            "5361;A woman in my family who had a heart attack is still a mother in a very sad way. It's really hard, you can't make it up.;Cynthia Davis;agenda\n",
            "5362;I've said it once - I think it's hard to talk about. Sometimes, when you're younger and you're older, it feels like it's just a part of your life that you're only a kid.;Ellen Page;agenda\n",
            "5363;I don't have a very great sense of humor. I don't have any - so maybe one day I'll start to. I don't know.;David Fincher;agenda\n",
            "5364;I've never loved a story, I'd rather have a good one. I like comedy - that's it. What do I love now? I'd much rather be on a TV show. I guess that's the only thing I'm more excited about.;Barry White;agenda\n",
            "5365;In terms of my own future, I feel like I have no problem with the people. I see how the world is getting a little bit more connected to music, movies, music in general, that I have a little bit of time for my own things.;Josh Homme;agenda\n",
            "5366;I'm going to make a film about my whole life and it will go on to feature the next five movies. It's in my head right now. I don't think about it anymore. That's the best way I can look at it.;Josh Homme;agenda\n",
            "5367;I've always been able to play with my mom, always being able to talk about music and how my mom used to sing at a lot of shows.;Branzo Gracie;agenda\n",
            "5368;I think there is definitely an opportunity to see a movie when I'm younger.;Lori Tomlin;agenda\n",
            "5369;I think music is probably one of the most important cultural institutions in our country, and when people are listening to music, and listening to it - those who have been exposed to that in a way that is unique, they know that they are going to find the beauty of the music they're looking for in their music. The fact that people have the patience to actually listen helps give them a chance to work out some new music, but there is still a way to go.;L.A.-based musician;music\n",
            "5370;It has been a lot of work and a lot of pain, having to be around a lot of people, to write songs. I mean, it's just too hard to write songs. It doesn't feel natural.;Lila Temple;agenda\n",
            "5371;I'm a rock-and-roll singer, that's what I do. But it's not my style. I'm very good with it. It's something I take from, and I'm very, very protective of my music.;Jagger;agenda\n",
            "5372;That is really a great thing to say: I\n",
            "\n",
            "[210 | 514.41] loss=2.19 avg=2.33\n",
            "[220 | 538.32] loss=2.13 avg=2.32\n",
            "[230 | 562.25] loss=2.28 avg=2.32\n",
            "[240 | 586.12] loss=2.22 avg=2.31\n",
            "[250 | 610.04] loss=2.28 avg=2.31\n",
            "[260 | 634.03] loss=2.45 avg=2.32\n",
            "[270 | 658.14] loss=2.52 avg=2.32\n",
            "[280 | 682.24] loss=2.24 avg=2.32\n",
            "[290 | 706.27] loss=2.24 avg=2.32\n",
            "[300 | 730.32] loss=2.24 avg=2.31\n",
            "[310 | 754.35] loss=2.14 avg=2.31\n",
            "[320 | 778.32] loss=2.19 avg=2.30\n",
            "[330 | 802.30] loss=2.20 avg=2.30\n",
            "[340 | 826.25] loss=2.17 avg=2.30\n",
            "[350 | 850.21] loss=2.23 avg=2.29\n",
            "[360 | 874.12] loss=2.40 avg=2.30\n",
            "[370 | 898.00] loss=2.44 avg=2.30\n",
            "[380 | 921.87] loss=2.33 avg=2.30\n",
            "[390 | 945.75] loss=2.09 avg=2.30\n",
            "[400 | 969.69] loss=2.13 avg=2.29\n",
            "======== SAMPLE 1 ========\n",
            " don't get paid for it.<|endoftext|>\n",
            "<|startoftext|>I love music. It gives me the freedom to say what I want.<|endoftext|>\n",
            "<|startoftext|>I'm also very blessed, that my family and friends put so much effort into not being a failure at all. It's really something I want to pursue, so I always wanted to be a professional, whatever that is all about.<|endoftext|>\n",
            "<|startoftext|>A great many children can do more than just pick a direction but have that same talent and intelligence and personality that they once had. And then, as a child, they are able to be a complete person.<|endoftext|>\n",
            "<|startoftext|>When I met God, there had been a time when I could be a part of another life that would be really amazing.<|endoftext|>\n",
            "<|startoftext|>I have always wanted to write a few songs. It's the hardest job in the world if you've worked there.<|endoftext|>\n",
            "<|startoftext|>The secret is to keep your fingers firmly in your ears, like any other small boy who will try to sound funny. If they can, just keep repeating.<|endoftext|>\n",
            "<|startoftext|>The human heart never gets old.<|endoftext|>\n",
            "<|startoftext|>There is no such thing as a truly great artist when you cannot achieve it, it is an instrument of the soul.<|endoftext|>\n",
            "<|startoftext|>The truth is, as you start working with this instrument, your mind grows more developed.<|endoftext|>\n",
            "<|startoftext|>The art which you learn in the first instance is great if you first see it. Then the next morning you will have done it.<|endoftext|>\n",
            "<|startoftext|>Every art which has been made by the first generation has proved its age.<|endoftext|>\n",
            "<|startoftext|>A man in love must feel the force of the moment, but always feel the passion.<|endoftext|>\n",
            "<|startoftext|>The most beautiful thing about life is that we live in it, whether its the art or its the economy.<|endoftext|>\n",
            "<|startoftext|>The beauty of life comes about through the interaction of people, and not through the interaction of material objects.<|endoftext|>\n",
            "<|startoftext|>The art of taking what you're given and making it your own.<|endoftext|>\n",
            "<|startoftext|>The art of loving is to feel the love which we have.<|endoftext|>\n",
            "<|startoftext|>There's a difference between being a part of something and being a part of it in life. There's a lot of love. A lot of good times. And if you can get those good times, you're a really great person.<|endoftext|>\n",
            "<|startoftext|>In my life, when things seem to me to be so small and you're trying to bring them to life and you're trying to take them and then you're not sure, then something is really wrong. The idea of, at best, nothing but an individual person.<|endoftext|>\n",
            "<|startoftext|>I used to hate being called a 'dude.' But not being called a woman is a form of misogyny. You don't get to do the dishes and be like, 'What are you eating, bro? What are you doing here?'<|endoftext|>\n",
            "<|startoftext|>I have a special talent called my 'dude.' When I'm working with women, you have to be very emotional, because you may not feel anything with respect to your work, but it's amazing.<|endoftext|>\n",
            "<|startoftext|>The art of getting good looks is so much more than just being famous. You live your life, you create your life. When you get good looks in front of your friends and fans, it's very satisfying.<|endoftext|>\n",
            "<|startoftext|>Being the first star in television, I was very fortunate.<|endoftext|>\n",
            "<|startoftext|>The human mind is like a giant bowl when one sits down to take a bite. When people see that the human mind sees a large bowl in front of it, they will look and think about how they did it, and be amazed that they didn't.<|endoftext|>\n",
            "<|startoftext|>The more you eat, the more you feel, feel what you are.<\n",
            "\n",
            "[410 | 1002.85] loss=2.37 avg=2.29\n",
            "[420 | 1026.83] loss=2.09 avg=2.29\n",
            "[430 | 1050.82] loss=2.23 avg=2.29\n",
            "[440 | 1074.74] loss=2.14 avg=2.28\n",
            "[450 | 1098.73] loss=2.06 avg=2.28\n",
            "[460 | 1122.83] loss=1.88 avg=2.26\n",
            "[470 | 1146.85] loss=2.40 avg=2.27\n",
            "[480 | 1170.87] loss=2.43 avg=2.27\n",
            "[490 | 1194.92] loss=2.00 avg=2.27\n",
            "[500 | 1218.92] loss=2.05 avg=2.26\n",
            "Saving checkpoint/run1/model-500\n",
            "[510 | 1245.55] loss=2.19 avg=2.26\n",
            "[520 | 1269.67] loss=2.09 avg=2.25\n",
            "[530 | 1293.61] loss=2.21 avg=2.25\n",
            "[540 | 1317.45] loss=2.33 avg=2.25\n",
            "[550 | 1341.46] loss=2.14 avg=2.25\n",
            "[560 | 1365.53] loss=1.88 avg=2.24\n",
            "[570 | 1389.55] loss=2.03 avg=2.24\n",
            "[580 | 1413.60] loss=2.28 avg=2.24\n",
            "[590 | 1437.58] loss=1.91 avg=2.23\n",
            "[600 | 1461.57] loss=1.97 avg=2.23\n",
            "======== SAMPLE 1 ========\n",
            "startoftext|>It's the only way to make money, not the other way around. And the only way to make money is to live your life in a way that makes it sustainable.<|endoftext|>\n",
            "<|startoftext|>The biggest advantage in a business is it takes time for growth to happen. I mean, I'm learning that the most valuable parts are taking delivery of the biggest parts, and that's a pretty big advantage in a business. So the things that you really want to accomplish are always in the past, and you can't take the past too far.<|endoftext|>\n",
            "<|startoftext|>We don't have to go out and create a business in order to be successful, but I do have the experience and the experience of having some success, and having some great ideas, and knowing that there's work to be done.<|endoftext|>\n",
            "<|startoftext|>I have been very lucky to have my family and I've always been able to be a part of such a big family. It's been an interesting journey for me and I'm blessed to still be blessed.<|endoftext|>\n",
            "<|startoftext|>The idea of going overseas for more than a few years to be home-grown and to be able to do your business as a small studio is one thing, and doing your business as a restaurant or a food restaurant in the back of the studio. I love working with these people. They're such artists. I love being in front of the camera.<|endoftext|>\n",
            "<|startoftext|>The idea that we have jobs for those who have talent is just another excuse to go overseas.<|endoftext|>\n",
            "<|startoftext|>A person who is able to make money from selling drugs and alcohol and have his life not deteriorate if he knows that his money is going to be used to cure cancer doesn't need to take their business overseas.<|endoftext|>\n",
            "<|startoftext|>The idea of going out and doing one thing without getting in trouble was just a bit crazy. It got old. People were like, 'What the heck, you have to get up and do that'.<|endoftext|>\n",
            "<|startoftext|>I don't think there's anything in business in which one's money will do much except your work.<|endoftext|>\n",
            "<|startoftext|>If you've got an organization that doesn't provide the ability to innovate, you should be able to compete on both the level of business and the level of innovation.<|endoftext|>\n",
            "<|startoftext|>I'm not interested in having my name in any of those industries because of the power of the market in the business.<|endoftext|>\n",
            "<|startoftext|>If I've got the opportunity to have my cake and eat it, I'm just going to make sure it's healthy in my own way. Just be aware of the fact that I still want to be there. But, there's always a day or two where if you lose weight, you go home and have a good time.<|endoftext|>\n",
            "<|startoftext|>I was just a little kid - and I was only 14 when this show came on - and my parents came over and we were always in the studio recording our songs. It was just the coolest time.<|endoftext|>\n",
            "<|startoftext|>When you're an owner and you feel you've got that ability to do the job you love, you do an unbelievable amount of work with those people. It's just such an amazing group.<|endoftext|>\n",
            "<|startoftext|>And also, I can tell you from personal experience, one of the greatest things I've ever witnessed - and just to take a joke - a lot of people start doing odd jobs where they're doing weird jobs. And I'm one of them. Because, you know, it's always been a job that's been fun to work with me.<|endoftext|>\n",
            "<|startoftext|>I love the way they're putting together the perfect picture. It's a little bit embarrassing because it's been done so many times before and it's one thing you never know - when the perfect picture is coming in, but you can't wait to use it - but it's a lovely thing to do because you never know until you get there.<|endoftext|>\n",
            "<|startoftext|>But once I made that cut, I had the luxury of seeing a ton of girls play the guitar. It was one of the greatest moments of my life.<|endoftext|>\n",
            "<|startoftext|>If you want the world to know that you love yourself that you're a\n",
            "\n",
            "[610 | 1494.79] loss=2.04 avg=2.22\n",
            "[620 | 1518.78] loss=2.06 avg=2.22\n",
            "[630 | 1542.79] loss=1.90 avg=2.21\n",
            "[640 | 1566.80] loss=2.19 avg=2.21\n",
            "[650 | 1590.81] loss=2.07 avg=2.21\n",
            "[660 | 1614.80] loss=2.10 avg=2.21\n",
            "[670 | 1638.77] loss=2.07 avg=2.20\n",
            "[680 | 1662.71] loss=2.11 avg=2.20\n",
            "[690 | 1686.62] loss=2.01 avg=2.20\n",
            "[700 | 1710.49] loss=2.19 avg=2.20\n",
            "[710 | 1734.40] loss=2.01 avg=2.19\n",
            "[720 | 1758.31] loss=2.14 avg=2.19\n",
            "[730 | 1782.25] loss=2.08 avg=2.19\n",
            "[740 | 1806.22] loss=1.93 avg=2.19\n",
            "[750 | 1830.19] loss=2.18 avg=2.19\n",
            "[760 | 1854.21] loss=2.05 avg=2.18\n",
            "[770 | 1878.23] loss=1.98 avg=2.18\n",
            "[780 | 1902.25] loss=1.94 avg=2.17\n",
            "[790 | 1926.33] loss=2.17 avg=2.17\n",
            "[800 | 1950.36] loss=1.89 avg=2.17\n",
            "======== SAMPLE 1 ========\n",
            " perform the best work of his life.<|endoftext|>\n",
            "<|startoftext|>I never felt comfortable in any form of communication in my whole life. From then on, I would be silent. I'd just say, 'I'll go play golf, and then I'll go to the office and they'll send me that tape.'<|endoftext|>\n",
            "<|startoftext|>My husband and I live in New York City and we've been married the last three years without any problems.<|endoftext|>\n",
            "<|startoftext|>I've always had these amazing relationships with my family. We've had such a positive relationship but we weren't always able to get along well. But our family is so wonderful.<|endoftext|>\n",
            "<|startoftext|>I have a lot of respect for my mom. She was the first person I ever met who was in love with someone and let them have their moments together when they were at their best. She's a wonderful person, and she's a wonderful wife. She's a wonderful mother, but she holds her own.<|endoftext|>\n",
            "<|startoftext|>I was a really fortunate, lucky girl. I have been blessed the number of parents, friends and students who have instilled faith in me, and who have allowed me the opportunity to be the most important person in my family without putting it on the back burner.<|endoftext|>\n",
            "<|startoftext|>I was just a kid at a very young age, and a lot of things were really, really simple. I got my first car, my first book, a movie of my own. That was it. Nothing more. It was all that mattered.<|endoftext|>\n",
            "<|startoftext|>I've had a number of bad experiences in my life, but I had never been in the business and had never been sober. So it's just interesting to watch something that you've never done in your life explode through this experience.<|endoftext|>\n",
            "<|startoftext|>It's wonderful to have a kid to talk to and a father to take care of and be around.<|endoftext|>\n",
            "<|startoftext|>You've got to be careful not to be a selfish person. What I'm trying to say is this. We see all of those moments, but we also never seem to forget them.<|endoftext|>\n",
            "<|startoftext|>I was born into a rich, hard-working family. I've always believed that it's my responsibility to educate myself to be a good dad.<|endoftext|>\n",
            "<|startoftext|>I'm a really happy person. It's amazing to wake up and be able to experience music in a very positive way that you haven't felt it yet.<|endoftext|>\n",
            "<|startoftext|>It's amazing to wake up every day, and to not worry about what's next for you, like when you go to sleep every morning. You've never felt that.<|endoftext|>\n",
            "<|startoftext|>I'm a great husband, have three dogs and a wife.<|endoftext|>\n",
            "<|startoftext|>I don't really do it in comedy. I'm a very serious guy - I know I could probably do some comedy, but I'm not really that serious. I like to be relaxed and to be able to laugh. That's what's important.<|endoftext|>\n",
            "<|startoftext|>I think there's an important lesson that anyone of any background can learn, even if they are a writer, by just being very well prepared.<|endoftext|>\n",
            "<|startoftext|>People always seem to be so protective of your music, and I think they're going for that because it's the most important thing in their lives.<|endoftext|>\n",
            "<|startoftext|>When I was a kid, my brother and my grandmother were there every night for me. They were the ones who always made me want to go out each night and make my day, and they always provided that warmth to me.<|endoftext|>\n",
            "<|startoftext|>I love being in the spotlight. It's nice to be in the spotlight. I don't think of myself as an actress, and yet it's something I can't wait to do.<|endoftext|>\n",
            "<|startoftext|>The good of any work is less important than its beauty.<|endoftext|>\n",
            "<|startoftext|>You'll have different jobs, different industries but you'll get better access to the services of higher levels of government. And when you do that, you'll be able to achieve more and\n",
            "\n",
            "[810 | 1983.53] loss=2.13 avg=2.17\n",
            "[820 | 2007.49] loss=2.28 avg=2.17\n",
            "[830 | 2031.36] loss=2.02 avg=2.17\n",
            "[840 | 2055.31] loss=2.06 avg=2.17\n",
            "[850 | 2079.60] loss=2.13 avg=2.17\n",
            "[860 | 2103.65] loss=2.01 avg=2.16\n",
            "[870 | 2127.53] loss=1.98 avg=2.16\n",
            "[880 | 2151.65] loss=2.03 avg=2.16\n",
            "[890 | 2175.75] loss=1.91 avg=2.15\n",
            "[900 | 2199.69] loss=2.15 avg=2.15\n",
            "[910 | 2223.61] loss=2.16 avg=2.15\n",
            "[920 | 2247.57] loss=2.05 avg=2.15\n",
            "[930 | 2271.55] loss=2.31 avg=2.15\n",
            "[940 | 2295.52] loss=2.04 avg=2.15\n",
            "[950 | 2319.51] loss=2.01 avg=2.15\n",
            "[960 | 2343.49] loss=2.13 avg=2.15\n",
            "[970 | 2367.49] loss=1.97 avg=2.15\n",
            "[980 | 2391.49] loss=1.98 avg=2.14\n",
            "[990 | 2415.47] loss=1.95 avg=2.14\n",
            "[1000 | 2439.42] loss=2.05 avg=2.14\n",
            "Saving checkpoint/run1/model-1000\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cbe_807_TXsg",
        "outputId": "f118debd-8231-4d26-98c0-54a02eb15df9"
      },
      "source": [
        "sess_2 = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess_2,\n",
        "              dataset=file_name,\n",
        "              model_name='124M',\n",
        "              steps=1000,\n",
        "              learning_rate = .02,\n",
        "              restore_from='fresh',\n",
        "              run_name='run3',\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=500\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  5.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 2302654 tokens\n",
            "Training...\n",
            "[10 | 50.16] loss=12.51 avg=12.51\n",
            "[20 | 95.21] loss=9.92 avg=11.20\n",
            "[30 | 140.58] loss=7.31 avg=9.89\n",
            "[40 | 185.96] loss=7.22 avg=9.22\n",
            "[50 | 231.38] loss=5.94 avg=8.55\n",
            "[60 | 276.84] loss=5.25 avg=7.98\n",
            "[70 | 322.24] loss=5.08 avg=7.56\n",
            "[80 | 367.64] loss=5.19 avg=7.25\n",
            "[90 | 413.16] loss=5.13 avg=7.00\n",
            "[100 | 458.68] loss=4.97 avg=6.79\n",
            "[110 | 504.22] loss=4.89 avg=6.61\n",
            "[120 | 549.71] loss=5.07 avg=6.48\n",
            "[130 | 595.17] loss=5.09 avg=6.36\n",
            "[140 | 640.66] loss=6.37 avg=6.36\n",
            "[150 | 686.13] loss=5.52 avg=6.30\n",
            "[160 | 731.65] loss=5.17 avg=6.23\n",
            "[170 | 777.11] loss=4.80 avg=6.14\n",
            "[180 | 822.57] loss=5.11 avg=6.07\n",
            "[190 | 868.06] loss=4.82 avg=6.00\n",
            "[200 | 913.50] loss=4.81 avg=5.94\n",
            "======== SAMPLE 1 ========\n",
            " going by and to it is it. We I'd it'm. When can I. can can was you you's they that a I and I is I in was you'd the it\n",
            "\n",
            "It is to I the lot there it.<|>My in there have a but the lot, not be a the we not to is it\n",
            "<|>\n",
            "<|>\n",
            "<oftext|>\n",
            "<|startoftext|>\n",
            "<|<|>\n",
            "<|>The of I the to have my and the and do that of not really we had's in world, I is to my which. To have in It of you I as the which that I is a but good. I can we know of I and get no it that to I would I was, to the 'I had of time. death. It, is a. It is there. We'd, is we I is and that to all that can II're I don is a I I have not a great a but they you have that get to good but have your that'd a and been a health as an a 'It's me on a a ''s a best, that I'd've of the good and life that would me is death and the the a and do not the health and have been, the it can be been in be best best my a in be the I do and just always always a the a not a. be a dad of my a I that and a the that do and. It and no the all no do who'm you to I had know and to I have life and do be a or a me have the best the me't the good. When that'd the life. He think in great the dad I don in a morning mom. It was life the be the the an dad to get I don and be a life,.<|startoftext|>\n",
            "<|>\n",
            "<|>\n",
            "<|startoftext|>\n",
            "<|<|startoftext|>\n",
            "<|>\n",
            "<|startoftext|startoftext|>\n",
            "<|endoftext|>We was be you's health.<|educationoftext|>\n",
            "<|>I that I. But it the my in I get a and time. The and a and the that not get, there has an, of a lot. That.<|startoftext|>\n",
            "25uel have, that a and a death to a it think, and a be not dad's and be was've you's death, to you is not the a that.\n",
            "<|startoftext|>I'm I are a not all it the life is to go that my you They is and you's the the time to the a a health and great life of and you've the 'Morning,. But to I a my life.<|startoftext|>\n",
            "<|>\n",
            "<|>\n",
            "<|>\n",
            "<of>We would the the dad is to be,oftext|>But be was government the health but it\n",
            " I've it are the good being, my lot for more go a mom on it would really not've you're think,. It are,, you is do just think you want in work you have a your it to was to their be and mom's death we are we never the I can've get we're the that not a, I was an the I I be been my. It.<|startoftext|startoftext|endoftext|>\n",
            "<|>A your me'd a to a,-. It.<|>\n",
            "<|endoftext|startoftext|>\n",
            "<|>You, and a good a I'd, I is have it's a the and'm. My my a I have and do that to dad that get a home, time. He was have I I that it've a health and that, is the life is the it of it that to the for a my what can'm think just've never, a the life is not my that't's's really, of the not have the and an like. It is and know not great been I of mom's go that go it's have a you would be my health and The world. You are a me was you a I. I and I'm all the good good a it.<|startoftext|endoftext|startoftext|startoftext|endoftext|startoftext|endoftext|>Mar to was be a music, then and So the lot or think of the I be's never a. It will I's and. So who don and the a not and think.<|endoftext|>\n",
            "<|>\n",
            "<|>We was own that you was the that that what I, the it. The and's I you the go in, a but I are a and It I think a the we and been the but have,\n",
            "\n",
            "[210 | 976.36] loss=4.43 avg=5.86\n",
            "[220 | 1021.84] loss=5.53 avg=5.84\n",
            "[230 | 1067.35] loss=4.99 avg=5.80\n",
            "[240 | 1112.79] loss=5.12 avg=5.77\n",
            "[250 | 1158.25] loss=4.49 avg=5.71\n",
            "[260 | 1203.69] loss=4.43 avg=5.65\n",
            "[270 | 1249.14] loss=4.36 avg=5.60\n",
            "[280 | 1294.63] loss=4.64 avg=5.56\n",
            "[290 | 1340.09] loss=4.57 avg=5.52\n",
            "[300 | 1385.54] loss=4.74 avg=5.49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrQNDRd0GD1X"
      },
      "source": [
        "Model Size: 124M: Modified Learning Rate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "dqKlONPThOcD",
        "outputId": "87cfd0e8-bc0d-4904-f496-6e45f906abcd"
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c109d49b8a61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_checkpoint_to_gdrive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'run1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'gpt2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYa12IkRhODd"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXSuTNERaw6K"
      },
      "source": [
        "After the model is trained, you can copy the checkpoint folder to your own Google Drive.\n",
        "\n",
        "If you want to download it to your personal computer, it's strongly recommended you copy it there first, then download from Google Drive. The checkpoint folder is copied as a `.rar` compressed file; you can download it and uncompress it locally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxeTYshJ_BcC"
      },
      "source": [
        "Model Size: 124M and 355M"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "UmODXVIc_JxS",
        "outputId": "6454a4fe-cb2e-4e32-8127-7b9b6cede32f"
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"355M\")\n",
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='355M',\n",
        "              steps=1000,\n",
        "              restore_from='fresh',\n",
        "              run_name='run2',\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=500\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 245Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:01, 713kit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 286Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001:  15%|██▍              | 207M/1.42G [00:38<03:45, 5.37Mit/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-8f16e722e52b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_gpt2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"355M\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_tf_sess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m gpt2.finetune(sess,\n\u001b[1;32m      5\u001b[0m               \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mdownload_gpt2\u001b[0;34m(model_dir, model_name)\u001b[0m\n\u001b[1;32m     91\u001b[0m                                     \u001b[0msub_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msub_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                                     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                                     file_name=file_name)\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mdownload_file_with_progress\u001b[0;34m(url_base, sub_dir, model_name, file_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m                   total=file_size, unit_scale=True) as pbar:\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDOWNLOAD_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDOWNLOAD_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3"
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJgV_b4bmzd"
      },
      "source": [
        "You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L"
      },
      "source": [
        "## Load a Trained Model Checkpoint\n",
        "\n",
        "Running the next cell will copy the `.rar` checkpoint file from your Google Drive into the Colaboratory VM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_VAL947fyEr"
      },
      "source": [
        "CheckPoint from 124M GPT-2 Model Size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD"
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='run1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTa6zf3e_9gV"
      },
      "source": [
        "The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "26551e90-39b3-408c-ed09-2a9c65847183"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='run1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-42fe8a91f36d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_tf_sess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_gpt2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'run1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gpt2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83bfb31c-46a9-417a-88ef-7a72ee175311"
      },
      "source": [
        "gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<|startoftext|>My biggest fear is if I don't have a good job. If I don't have a good job, I don't know how I can get through the day.<|endoftext|>\n",
            "<|startoftext|>Women don't have to look at the man of the people. They can have a good job, they can have a good marriage, they can have a good life and, if they're lucky, they can have a great time.<|endoftext|>\n",
            "<|startoftext|>If you are a soldier and you have a good job, you have to be a soldier yourself. If you don't have a soldier, there's nothing you can do. If you don't have a soldier, there's nothing you can do.<|endoftext|>\n",
            "<|startoftext|>I think I was the youngest of the bunch, and I was a soldier. I had my mother and father come over and support me. It was a very, very tough time.<|endoftext|>\n",
            "<|startoftext|>The general concept of a good job, for me, is that it saves money, that it provides an outlet for a person to achieve their potential. I don't think that there is a price which should be charged for doing that.<|endoftext|>\n",
            "<|startoftext|>I love being a wife and having the opportunity to have a child. I don't know why. It's just a really, really good feeling to have a child and I love my wife and I'm really blessed to have a child and I'm so happy that I have a child and I'm so happy that I have a good job. And I think that that's really good for me.<|endoftext|>\n",
            "<|startoftext|>It's funny, because I've always been a professional and I've been passionate about the sport. I've been doing sports since I was a kid and I've done everything. I've been involved in the game. I've been involved in the business. I've been involved in the family. Being able to be able to have a good job and be able to do well in the business is really good.<|endoftext|>\n",
            "<|startoftext|>I don't want to be a doctor or a lawyer, I want to be a good father and a good husband.<|endoftext|>\n",
            "<|startoftext|>The good old days of the American people is over.<|endoftext|>\n",
            "<|startoftext|>I like to do a lot of things. I like to do a lot of things. I like to do a lot of things. I like to do a lot of things. I like to do a lot of things.<|endoftext|>\n",
            "<|startoftext|>I like the idea of being able to do anything in this business. I love to do movies. I love to do movies.<|endoftext|>\n",
            "<|startoftext|>I've done all kinds of things, but it was a problem of being on time. I was on time. I was on time. I was on time. I have a bad memory. I was on time. I had a bad attitude.<|endoftext|>\n",
            "<|startoftext|>I love to love and be loved. I can be loved for anything and everything. And I don't care about whether it's good or bad, I'm just loving and being loved by everyone.<|endoftext|>\n",
            "<|startoftext|>I've trained my body. I train my mind. I train my body. I've trained my mind. And I know that I will make good use of every moment.<|endoftext|>\n",
            "<|startoftext|>I love being a mom. I have a job. I have a family. I have a home. I have a job. I have a family. I have a job. I have a job. I have a family. I have a job. I have a job. I have a job. I have a job. I have a job. I have a job. I have a job. I have a job. I have a job. I have a job. I have a job. I have a job. I have a job. I have a job. I have a job. I have a job.<|endoftext|>\n",
            "<|startoftext|>I don't want to be an actor. I want to be a mom. I don't want to be an actress because I don't want to be an actor when my job is to train by myself.<|endoftext|>\n",
            "<|startoftext|>I'm not the sort of person who would go out and say, '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
        "\n",
        "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
        "\n",
        "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
        "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
        "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f70427e9-31cb-40ba-d4de-7b94c7e6b55d"
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              length=250,\n",
        "              temperature=1.0,\n",
        "              prefix=\"LORD\",\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LORD GALLAN\n",
            "2036;For I say only is intellectual knowledge, when Spinoza is quoted as saying that public opinion had anything to do with its liberation from oppression by colonialism and that it had nothing to do with its restoration to the institutions of higher learning.; Leonardo da Vinci;knowledge\n",
            "2037;Learning mathematics, inasmuch as I know nothing or am not a mathematician, declares me the myth of mastery of Nature.;Gertrude Stein;knowledge\n",
            "2038;I am bound to observe that all men invent a different mental concept than above. My imaginary mind may yet, and perhaps naturally, as it have had a subtle involvement with an assumed world, in the course of its persistence obscure its subconscious knowledge in the intellectual advancement characteristic a man of good ideas.;John Stuart Mill;knowledge\n",
            "2039;It is necessary that we be able to choose the Supreme Being who has given us knowledge of everything Therefore it is unjust that we should not seek to learn.;Joseph Ratzinger;knowledge\n",
            "2040;What is it about literature? Well, it's about revolutionary knowledge.;Irving Howe;knowledge\n",
            "2041;I notice this attitude to history. I go to university very often, and I've learned\n",
            "====================\n",
            "LORD - This kind of friendship is not in the nature of friendship, but of quarrels like ours one fritter away at all. If he has any inside it is all a game to get the mark out. It's like trying to slip into a big black hole. It's temporary and unhealthy.;Safari Feldman;hope\n",
            "16513;He was also a long distance runner. I noticed stuff on the Internet, but then I became interested in becoming an actress and then hopefully getting married. He said you could ride straight back to Boston right there.;Idris Elba;hope\n",
            "15344;I think my hope and my hope is that in music and video game music, comics, and especially as far as I can see, essentially, these are short, novelty, very specific art forms that resonate. It's not so much in the prize money for artists they might be competing for a small slice in prize money. That's where the money's going to go.;Cabaret;hope\n",
            "15345;In the films of 2004 under Bill Clinton, Bill Clinton had a foreign policy, until he was up four years, and a story about gay marriage and amnesty shows up in almost the same scope\n",
            "====================\n",
            "LORD, N.C. - Though the SARS virus showed no signs of life and its enormous killer cities, the world was stunned at the terrifying similarity to that of the SARS shown thus far to the SARS outbreak in Britain in 1948. These charts, pages 360-377, have been circulating freely in the scientific literature for 15 years, and they demonstrate just how sensitive scientists are to the technical differences between today's humans and the SARS patients.<|endoftext|>\n",
            "<|startoftext|>I don't know much about dry con artists, but I know a lot about working with radioactive material because of my old graduate research group, and because FORD very publicly fought the full effect of Lothsand's practice on the climate of environmental awareness.<|endoftext|>\n",
            "<|startoftext|>Deodorant is much, if not more important than toothpaste in convincing people that names change places and that chemicals are not the same as fragrance.<|endoftext|>\n",
            "<|startoftext|>As a child, I saw plenty of women roller skating and clambering along on giant egg-shaped crests, gliding down hills in dreadlocks and riding clams for\n",
            "====================\n",
            "LORD EYERS: I talk to my son on The National. I see his relationship and he's working on his other project. What can he tell me of mine? [] Stephen King: All fans know my dad. I just wish I were like him. He gave me an amazing upbringing. What I've learned through that experience so far is that kids need their dad to be a success. Being supportive takes a toll on your ego. So for me being a dad was the most amazing thing.<|endoftext|>\n",
            "<|startoftext|>Because my dad rarely gets up at 5 or 6 and stays in bed at night, I sort of tended to sleep on until he was warmer.<|endoftext|>\n",
            "<|startoftext|>I waited until school started. My dad saved from school and everything.<|endoftext|>\n",
            "<|startoftext|>As far as my working life is concerned, I'm a dad. I make sure kids go to bed at night, but I do it my way.<|endoftext|>\n",
            "<|startoftext|>My dad was the most energetic dad that I ever knew.<|endoftext|>\n",
            "<\n",
            "====================\n",
            "LORD: Is it wrong that purity and truth are not in harmony?OUR PRINCESS OPPHAPTER XXIXY:\n",
            "I could not have any farther to go therethan also my banquet with Lord Henry with whom I thought to introduce myself before the people. An uncertain meeting of great intelligence suggested there being something rather disastrous in my relations with Almighty God. I resolved to witness again my freedom with God Nation because as I stand now upon the threshold of the threshold of God-made life I hope there is not a door which will not knock. Before I shall go any further, I desire give God proud serenity to show his utterance in my behalf, and to use the test of eternity to prove into what everlasting glory he really is.<|endoftext|>\n",
            "<|startoftext|>Later the meaning of old age might be lost, or intimacy lost, or the stable might be ruined, or even weakened.<|endoftext|>\n",
            "<|startoftext|>When one is old, one approaches war like a needle in a hat. The wounds inflicted on soldiers and civilians by war-making federal spending programs deeply harm traditional family life. But when one gets old, it is often as if\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2"
      },
      "source": [
        "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0"
      },
      "source": [
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-LRex8lfv1g"
      },
      "source": [
        "# may have to run twice to get file to download\n",
        "files.download(gen_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQAN3M6RT7Kj"
      },
      "source": [
        "## Generate Text From The Pretrained Model\n",
        "\n",
        "If you want to generate text from the pretrained model, not a finetuned model, pass `model_name` to `gpt2.load_gpt2()` and `gpt2.generate()`.\n",
        "\n",
        "This is currently the only way to generate text from the 774M or 1558M models with this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsUd_jHgUZnD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "4e0c8a3f-3527-41c4-e3fe-3357f3f8f6c2"
      },
      "source": [
        "model_name = \"774M\"\n",
        "\n",
        "gpt2.download_gpt2(model_name=model_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 354Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 131Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 279Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 3.10Git [00:23, 131Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 380Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 2.10Mit [00:00, 226Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 199Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAe4NpKNUj2C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "b09bfe1d-2ff8-4b8a-fffb-273d28d5d4ae"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.load_gpt2(sess, model_name=model_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0828 18:37:58.571830 139905369159552 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading pretrained model models/774M/model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xInIZKaU104",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "outputId": "56348e28-7d08-45e3-c859-f26c0efd066d"
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              model_name=model_name,\n",
        "              prefix=\"The secret of life is\",\n",
        "              length=100,\n",
        "              temperature=0.7,\n",
        "              top_p=0.9,\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The secret of life is that it's really easy to make it complicated,\" said Bill Nye, the host of the popular science show \"Bill Nye the Science Guy.\" \"And this is one of the reasons why we all need to be smarter about science, because we can't keep up with the amazing things that are going on all the time.\"\n",
            "\n",
            "While Nye is correct that \"everything that's going on all the time\" is making the world a better place, he misses the point. This is not\n",
            "====================\n",
            "The secret of life is in the rhythm of the universe. It's not a mystery. It's not a mystery to me. It's the nature of the universe. It's the beauty of the universe. It's the way the universe works. It's the way the universe is. It's the way the universe is going to work. It's the way the universe is. It's the way the universe is. It's the way the universe is. It's the way the universe is. It's the way\n",
            "====================\n",
            "The secret of life is in the universe.\n",
            "\n",
            "\n",
            "-\n",
            "\n",
            "The Red Devil\n",
            "\n",
            "It's the end of the world as we know it, and the only thing that can save us is a band of super-powered individuals known as the Red Devil.\n",
            "\n",
            "\n",
            "The Red Devil is a group of super-powered individuals who are seeking the secret of life and the only way they know how to do it is by taking on the roles of a variety of different super-powered individuals, each of which has their own\n",
            "====================\n",
            "The secret of life is in the mixing of the elements, and it is the mixing of the elements that makes life possible.\"\n",
            "\n",
            "But in the world of food science, the idea of a \"complex\" or \"complexity\" is almost entirely imaginary.\n",
            "\n",
            "As a scientist, I'm fascinated by the question of how life first began.\n",
            "\n",
            "It's the question that drives my work and the work of the scientists who work on it.\n",
            "\n",
            "My current research is exploring how microbes work in the first moments\n",
            "====================\n",
            "The secret of life is the journey of life, the search for the truth.\n",
            "\n",
            "4.4.2. The last thing you know\n",
            "\n",
            "There is nothing more important than the last thing you know.\n",
            "\n",
            "4.4.3. The little things that make all the difference\n",
            "\n",
            "The little things that make all the difference.\n",
            "\n",
            "4.4.4. The truth is the best teacher\n",
            "\n",
            "The truth is the best teacher.\n",
            "\n",
            "4.4.5. The truth is what\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHiVP53FnsX"
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTXWNUygS5E"
      },
      "source": [
        "# LICENSE\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Max Woolf\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ]
}