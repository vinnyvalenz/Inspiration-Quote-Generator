{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "inspirational-quote-gen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Gcx5pgJoAug"
      },
      "source": [
        "############################################################\n",
        "#\tAll code taken from https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
        "#\tSome modifications have been made for the purppose of this project\n",
        "############################################################\n",
        "\n",
        "import numpy\n",
        "# import spacy\n",
        "# import gensim\n",
        "import sys\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
        "# nltk.download('punkt')\n",
        "# from gensim.models import KeyedVectors\n",
        "# from gensim.test.utils import common_texts\n",
        "# from gensim.models import Word2Vec\n",
        "# import en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3oc87nN93gx"
      },
      "source": [
        "file = open(\"mqd2.txt\").read()\n",
        "file = file.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPk6qbr7-B35"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
        "from nltk import FreqDist\n",
        "tok = RegexpTokenizer('\\w+|\\w+.')\n",
        "data = tok.tokenize(file)\n",
        "for w in data:\n",
        "  if w == 't' or w == 's':\n",
        "    data.remove(w)\n",
        "#data = data[0:int(len(data)/2)]\n",
        "#raw_data =\" \".join(data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_4UInVGep13"
      },
      "source": [
        "# freq = FreqDist(data)\n",
        "# for w in range(len(data)):\n",
        "#   if freq[data[w]] < 3:\n",
        "#     data[w] = 'other'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3DcyBKM-j5S"
      },
      "source": [
        "words = sorted(list(set(data)))\n",
        "#words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2MqHPd8ckj7",
        "outputId": "0f35977b-bbc5-4a50-91c2-62356db70997"
      },
      "source": [
        "n_words = len(data)\n",
        "print(n_words)\n",
        "n_vocab = len(words)\n",
        "print(n_vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "261788\n",
            "14590\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUuUjXfXCCB2"
      },
      "source": [
        "# nlp = en_core_web_lg.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzCHSP_QCcc1"
      },
      "source": [
        "# from gensim.models.keyedvectors import WordEmbeddingsKeyedVectors\n",
        "\n",
        "# wordList =[]\n",
        "# vectorList = []\n",
        "# for key in nlp.vocab.strings:\n",
        "#   wordList.append(key)\n",
        "#   vectorList.append(nlp.vocab[key].vector)\n",
        "\n",
        "# kv = WordEmbeddingsKeyedVectors(nlp.vocab.vectors_length)\n",
        "\n",
        "# kv.add(wordList, vectorList)\n",
        "# #kv.most_similar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aU2lIxddnO9"
      },
      "source": [
        "word_to_num = dict((c, i) for i, c in enumerate(words))\n",
        "#word_to_num"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaO1cdFZd54h",
        "outputId": "887ce1c5-412f-4535-f157-1eaa6648a5d0"
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 8\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_words - seq_length, 1):\n",
        "\tseq_in = data[i:i + seq_length]\n",
        "\tseq_out = data[i + seq_length]\n",
        "\tdataX.append([word_to_num[w] for w in seq_in])\n",
        "\tdataY.append(word_to_num[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  261780\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bibt84KrgIc2"
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "#print(X[0])\n",
        "#print(y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzWC38Qaw6E1"
      },
      "source": [
        "...\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(1024, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0kbdDJLxQPp"
      },
      "source": [
        "...\n",
        "# define the checkpoint\n",
        "filepath=\"iqg-checkpoint-s8-n1024.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzOXy4uUxU2L",
        "outputId": "c843aca4-a6be-492e-fcc4-66199ff91f68"
      },
      "source": [
        "model.fit(X, y, epochs=100, batch_size=256, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.3413\n",
            "\n",
            "Epoch 00001: loss improved from 0.35107 to 0.34128, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 2/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.3425\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.34128\n",
            "Epoch 3/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.3347\n",
            "\n",
            "Epoch 00003: loss improved from 0.34128 to 0.33471, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 4/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.3323\n",
            "\n",
            "Epoch 00004: loss improved from 0.33471 to 0.33228, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 5/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.3278\n",
            "\n",
            "Epoch 00005: loss improved from 0.33228 to 0.32784, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 6/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.3210\n",
            "\n",
            "Epoch 00006: loss improved from 0.32784 to 0.32100, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 7/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.3213\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.32100\n",
            "Epoch 8/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.3153\n",
            "\n",
            "Epoch 00008: loss improved from 0.32100 to 0.31528, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 9/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.3129\n",
            "\n",
            "Epoch 00009: loss improved from 0.31528 to 0.31295, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 10/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.3070\n",
            "\n",
            "Epoch 00010: loss improved from 0.31295 to 0.30705, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 11/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.3019\n",
            "\n",
            "Epoch 00011: loss improved from 0.30705 to 0.30188, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 12/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.3035\n",
            "\n",
            "Epoch 00012: loss did not improve from 0.30188\n",
            "Epoch 13/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2981\n",
            "\n",
            "Epoch 00013: loss improved from 0.30188 to 0.29811, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 14/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2959\n",
            "\n",
            "Epoch 00014: loss improved from 0.29811 to 0.29588, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 15/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2916\n",
            "\n",
            "Epoch 00015: loss improved from 0.29588 to 0.29164, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 16/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2862\n",
            "\n",
            "Epoch 00016: loss improved from 0.29164 to 0.28618, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 17/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2876\n",
            "\n",
            "Epoch 00017: loss did not improve from 0.28618\n",
            "Epoch 18/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2847\n",
            "\n",
            "Epoch 00018: loss improved from 0.28618 to 0.28468, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 19/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2860\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.28468\n",
            "Epoch 20/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2777\n",
            "\n",
            "Epoch 00020: loss improved from 0.28468 to 0.27772, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 21/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2750\n",
            "\n",
            "Epoch 00021: loss improved from 0.27772 to 0.27497, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 22/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2707\n",
            "\n",
            "Epoch 00022: loss improved from 0.27497 to 0.27066, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 23/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2713\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.27066\n",
            "Epoch 24/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2670\n",
            "\n",
            "Epoch 00024: loss improved from 0.27066 to 0.26700, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 25/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2653\n",
            "\n",
            "Epoch 00025: loss improved from 0.26700 to 0.26530, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 26/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2629\n",
            "\n",
            "Epoch 00026: loss improved from 0.26530 to 0.26291, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 27/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2657\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.26291\n",
            "Epoch 28/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2597\n",
            "\n",
            "Epoch 00028: loss improved from 0.26291 to 0.25967, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 29/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2601\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.25967\n",
            "Epoch 30/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2542\n",
            "\n",
            "Epoch 00030: loss improved from 0.25967 to 0.25419, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 31/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2538\n",
            "\n",
            "Epoch 00031: loss improved from 0.25419 to 0.25378, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 32/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2538\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.25378\n",
            "Epoch 33/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2506\n",
            "\n",
            "Epoch 00033: loss improved from 0.25378 to 0.25064, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 34/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2533\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.25064\n",
            "Epoch 35/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2465\n",
            "\n",
            "Epoch 00035: loss improved from 0.25064 to 0.24648, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 36/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2406\n",
            "\n",
            "Epoch 00036: loss improved from 0.24648 to 0.24064, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 37/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2466\n",
            "\n",
            "Epoch 00037: loss did not improve from 0.24064\n",
            "Epoch 38/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2371\n",
            "\n",
            "Epoch 00038: loss improved from 0.24064 to 0.23708, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 39/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2393\n",
            "\n",
            "Epoch 00039: loss did not improve from 0.23708\n",
            "Epoch 40/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2341\n",
            "\n",
            "Epoch 00040: loss improved from 0.23708 to 0.23414, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 41/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2382\n",
            "\n",
            "Epoch 00041: loss did not improve from 0.23414\n",
            "Epoch 42/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2384\n",
            "\n",
            "Epoch 00042: loss did not improve from 0.23414\n",
            "Epoch 43/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2353\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.23414\n",
            "Epoch 44/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2347\n",
            "\n",
            "Epoch 00044: loss did not improve from 0.23414\n",
            "Epoch 45/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2265\n",
            "\n",
            "Epoch 00045: loss improved from 0.23414 to 0.22650, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 46/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2290\n",
            "\n",
            "Epoch 00046: loss did not improve from 0.22650\n",
            "Epoch 47/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2309\n",
            "\n",
            "Epoch 00047: loss did not improve from 0.22650\n",
            "Epoch 48/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2267\n",
            "\n",
            "Epoch 00048: loss did not improve from 0.22650\n",
            "Epoch 49/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2261\n",
            "\n",
            "Epoch 00049: loss improved from 0.22650 to 0.22613, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 50/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2292\n",
            "\n",
            "Epoch 00050: loss did not improve from 0.22613\n",
            "Epoch 51/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2253\n",
            "\n",
            "Epoch 00051: loss improved from 0.22613 to 0.22525, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 52/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2226\n",
            "\n",
            "Epoch 00052: loss improved from 0.22525 to 0.22263, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 53/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2182\n",
            "\n",
            "Epoch 00053: loss improved from 0.22263 to 0.21825, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 54/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2224\n",
            "\n",
            "Epoch 00054: loss did not improve from 0.21825\n",
            "Epoch 55/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2173\n",
            "\n",
            "Epoch 00055: loss improved from 0.21825 to 0.21731, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 56/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2153\n",
            "\n",
            "Epoch 00056: loss improved from 0.21731 to 0.21529, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 57/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2148\n",
            "\n",
            "Epoch 00057: loss improved from 0.21529 to 0.21481, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 58/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2143\n",
            "\n",
            "Epoch 00058: loss improved from 0.21481 to 0.21426, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 59/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2150\n",
            "\n",
            "Epoch 00059: loss did not improve from 0.21426\n",
            "Epoch 60/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2170\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.21426\n",
            "Epoch 61/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2115\n",
            "\n",
            "Epoch 00061: loss improved from 0.21426 to 0.21147, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 62/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2112\n",
            "\n",
            "Epoch 00062: loss improved from 0.21147 to 0.21118, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 63/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2160\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.21118\n",
            "Epoch 64/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2078\n",
            "\n",
            "Epoch 00064: loss improved from 0.21118 to 0.20784, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 65/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2100\n",
            "\n",
            "Epoch 00065: loss did not improve from 0.20784\n",
            "Epoch 66/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2041\n",
            "\n",
            "Epoch 00066: loss improved from 0.20784 to 0.20410, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 67/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2079\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.20410\n",
            "Epoch 68/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2089\n",
            "\n",
            "Epoch 00068: loss did not improve from 0.20410\n",
            "Epoch 69/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2023\n",
            "\n",
            "Epoch 00069: loss improved from 0.20410 to 0.20225, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 70/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2039\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.20225\n",
            "Epoch 71/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2030\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.20225\n",
            "Epoch 72/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2012\n",
            "\n",
            "Epoch 00072: loss improved from 0.20225 to 0.20123, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 73/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1986\n",
            "\n",
            "Epoch 00073: loss improved from 0.20123 to 0.19863, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 74/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1999\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.19863\n",
            "Epoch 75/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.2038\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.19863\n",
            "Epoch 76/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1978\n",
            "\n",
            "Epoch 00076: loss improved from 0.19863 to 0.19780, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 77/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1987\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.19780\n",
            "Epoch 78/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1989\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.19780\n",
            "Epoch 79/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1929\n",
            "\n",
            "Epoch 00079: loss improved from 0.19780 to 0.19293, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 80/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1953\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.19293\n",
            "Epoch 81/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1962\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.19293\n",
            "Epoch 82/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1913\n",
            "\n",
            "Epoch 00082: loss improved from 0.19293 to 0.19129, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 83/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1925\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.19129\n",
            "Epoch 84/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1863\n",
            "\n",
            "Epoch 00084: loss improved from 0.19129 to 0.18633, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 85/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1902\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.18633\n",
            "Epoch 86/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1927\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.18633\n",
            "Epoch 87/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1903\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.18633\n",
            "Epoch 88/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1899\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.18633\n",
            "Epoch 89/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1889\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.18633\n",
            "Epoch 90/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1887\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.18633\n",
            "Epoch 91/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1839\n",
            "\n",
            "Epoch 00091: loss improved from 0.18633 to 0.18389, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 92/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1873\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.18389\n",
            "Epoch 93/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1866\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.18389\n",
            "Epoch 94/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1830\n",
            "\n",
            "Epoch 00094: loss improved from 0.18389 to 0.18302, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 95/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1825\n",
            "\n",
            "Epoch 00095: loss improved from 0.18302 to 0.18249, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 96/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1834\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.18249\n",
            "Epoch 97/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1859\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.18249\n",
            "Epoch 98/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1818\n",
            "\n",
            "Epoch 00098: loss improved from 0.18249 to 0.18182, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 99/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1805\n",
            "\n",
            "Epoch 00099: loss improved from 0.18182 to 0.18054, saving model to iqg-checkpoint-s8-n1024.hdf5\n",
            "Epoch 100/100\n",
            "1023/1023 [==============================] - 15s 15ms/step - loss: 0.1830\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.18054\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5aae2c9450>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRRN8lx4yYOF"
      },
      "source": [
        "...\n",
        "# load the network weights\n",
        "filename = \"iqg-model-seq3-200epochs-1024n.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x73iqycWyfBv"
      },
      "source": [
        "#from sklearn.metrics.pairwise import cosine_similarity\n",
        "num_to_word = dict((i, c) for i, c in enumerate(words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ4V0H9xykof",
        "outputId": "1c3d292e-6c96-4547-ac2d-3f55cffa78a3"
      },
      "source": [
        "\n",
        "...\n",
        "# pick a random seed\n",
        "# seed = \"if i had a dollar for every time\".split()\n",
        "# seed = [word_to_num[w] for w in seed]\n",
        "# pattern = seed\n",
        "a1 = []\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ' '.join([num_to_word[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(100):\n",
        "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    #print('\\n', get_top_n_words(prediction, 3))\n",
        "    #index = numpy.argmax(prediction)\n",
        "    index = random_top_n_word_index(prediction, 1)\n",
        "    result = num_to_word[index]+' '\n",
        "    seq_in = [num_to_word[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" days of america lying ahead it is the \"\n",
            "men and women in uniform who protect defend and make us proud to whom we should look and give thanks every night canadas the best country in the world next to acquiring good friends the best acquisition is that of good books the days you work are the best days sometimes you feel awkward being what youre best at you feel like you have to be something new the best men of the best epochs are simply those who make the fewest blunders and commit the fewest sins quit while youre ahead all the best gamblers do good manners will \n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biyWqOY3M2Tw"
      },
      "source": [
        "def get_top_n_words(arr, n):\n",
        "  arr_r = arr.reshape(-1)\n",
        "  top_n = arr_r.argsort()[-n:][::-1]\n",
        "  word_arr = [num_to_word[w] for w in top_n]\n",
        "  return word_arr\n",
        "\n",
        "def random_top_n_word_index(arr, n):\n",
        "  r = numpy.random.randint(0, n)\n",
        "  arr_r = arr.reshape(-1)\n",
        "  top_n = arr_r.argsort()[-n:][::-1]\n",
        "  return top_n[r]\n",
        "\n",
        "#print(a1)\n",
        "#print(get_top_n_words(a1 , 2))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}